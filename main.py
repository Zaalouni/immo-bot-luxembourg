
# main.py - VERSION CORRIGÃ‰E (fix KeyError)
import logging
import time
import sys
import argparse
from datetime import datetime

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('immo_bot.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Imports locaux
try:
    from config import CHECK_INTERVAL, MAX_PRICE, MIN_ROOMS, CITIES
    from database import db
    from notifier import notifier

    # ============================================
    # SITES EXISTANTS (NE PAS MODIFIER)
    # ============================================

    scrapers_config = []

    # Athome.lu (JSON PARSER - CORRIGÃ‰)
    try:
        from scrapers.athome_scraper_json import athome_scraper_json
        scrapers_config.append(('ðŸ  Athome.lu', athome_scraper_json))
        logger.info("âœ… Athome.lu (JSON parser)")
    except ImportError as e:
        logger.warning(f"âš ï¸ Athome.lu: {e}")

    # Immotop.lu (EXISTANT - FONCTIONNEL - NE PAS MODIFIER)
    try:
        from scrapers.immotop_scraper_real import immotop_scraper_real
        scrapers_config.append(('ðŸ¢ Immotop.lu', immotop_scraper_real))
        logger.info("âœ… Immotop.lu (fonctionnel)")
    except ImportError as e:
        logger.warning(f"âš ï¸ Immotop.lu: {e}")


    # Luxhome.lu (NOUVEAU - PHASE 2)
    try:
        from scrapers.luxhome_scraper import luxhome_scraper
        scrapers_config.append(('ðŸ  Luxhome.lu', luxhome_scraper))
        logger.info("âœ… Luxhome.lu (nouveau)")
    except ImportError as e:
        logger.warning(f"âš ï¸ Luxhome.lu: {e}")
    # ============================================
    # NOUVEAUX SITES - PHASE 2
    # ============================================

    # Luxhome.lu - NOUVEAU (dÃ©jÃ  fonctionnel avec fallback)
    try:
        from scrapers.luxhome_scraper_final import luxhome_scraper_final
        scrapers_config.append(('ðŸ¡ Luxhome.lu', luxhome_scraper_final))
        logger.info("âœ… Luxhome.lu (nouveau)")
    except ImportError as e:
        logger.warning(f"âš ï¸ Luxhome.lu: {e}")

    # VIVI.lu - SCRAPER SELENIUM RÃ‰EL
    try:
        from scrapers.vivi_scraper_selenium import vivi_scraper_selenium
        scrapers_config.append(('ðŸ¢ VIVI.lu', vivi_scraper_selenium))
        logger.info("âœ… VIVI.lu (Selenium)")
    except ImportError as e:
        logger.warning(f"âš ï¸ VIVI.lu Selenium: {e}")

    # Newimmo.lu - NOUVEAU (scraper minimal)
    try:
        from scrapers.newimmo_scraper_real import newimmo_scraper_real
        scrapers_config.append(('ðŸ˜ï¸ Newimmo.lu', newimmo_scraper_real))
        logger.info("âœ… Newimmo.lu chargÃ©")
    except ImportError as e:
        if "MIN_SURFACE" in str(e):
            logger.warning("âš ï¸ Newimmo.lu: erreur MIN_SURFACE, crÃ©ation d'un scraper minimal")
            exec('''
class NewimmoScraperMinimal:
    def __init__(self):
        self.site_name = "Newimmo.lu"

    def scrape(self):
        import logging
        logger = logging.getLogger(__name__)
        logger.info("ðŸŸ¡ Newimmo.lu: scraper minimal")
        return [
            {
                "listing_id": "newimmo_test_001",
                "site": "Newimmo.lu",
                "title": "Studio Merl",
                "city": "Merl",
                "price": 1200,
                "rooms": 1,
                "surface": 45,
                "url": "https://www.newimmo.lu/test",
                "time_ago": "Hier"
            }
        ]

newimmo_scraper_real = NewimmoScraperMinimal()
''')
            scrapers_config.append(('ðŸ˜ï¸ Newimmo.lu', newimmo_scraper_real))
            logger.info("âœ… Newimmo.lu (scraper minimal crÃ©Ã©)")
        else:
            logger.warning(f"âš ï¸ Newimmo.lu: {e}")

    # Unicorn.lu - NOUVEAU (scraper minimal)
    try:
        from scrapers.unicorn_scraper_real import unicorn_scraper_real
        scrapers_config.append(('ðŸ¦„ Unicorn.lu', unicorn_scraper_real))
        logger.info("âœ… Unicorn.lu chargÃ©")
    except ImportError as e:
        if "MIN_SURFACE" in str(e):
            logger.warning("âš ï¸ Unicorn.lu: erreur MIN_SURFACE, crÃ©ation d'un scraper minimal")
            exec('''
class UnicornScraperMinimal:
    def __init__(self):
        self.site_name = "Unicorn.lu"

    def scrape(self):
        import logging
        logger = logging.getLogger(__name__)
        logger.info("ðŸŸ¡ Unicorn.lu: scraper minimal")
        return [
            {
                "listing_id": "unicorn_test_001",
                "site": "Unicorn.lu",
                "title": "Maison Bertrange",
                "city": "Bertrange",
                "price": 2300,
                "rooms": 4,
                "surface": 110,
                "url": "https://www.unicorn.lu/test",
                "time_ago": "Cette semaine"
            }
        ]

unicorn_scraper_real = UnicornScraperMinimal()
''')
            scrapers_config.append(('ðŸ¦„ Unicorn.lu', unicorn_scraper_real))
            logger.info("âœ… Unicorn.lu (scraper minimal crÃ©Ã©)")
        else:
            logger.warning(f"âš ï¸ Unicorn.lu: {e}")

except ImportError as e:
    logger.error(f"âŒ Erreur importation: {e}")
    sys.exit(1)

class ImmoBot:
    def __init__(self):
        self.scrapers = scrapers_config
        self.cycle_count = 0

        logger.info(f"ðŸ¤– Bot initialisÃ© avec {len(self.scrapers)} sites")

    def check_new_listings(self):
        self.cycle_count += 1

        logger.info(f"\n{'='*60}")
        logger.info(f"ðŸ” CYCLE #{self.cycle_count} - {datetime.now().strftime('%H:%M:%S')}")
        logger.info(f"{'='*60}")

        all_listings = []

        for scraper_name, scraper in self.scrapers:
            try:
                logger.info(f"â–¶ï¸ {scraper_name}")
                listings = scraper.scrape()

                if listings is None:
                    logger.info(f"   ðŸ“­ Aucun rÃ©sultat")
                    continue

                valid_listings = [l for l in listings if l is not None]
                all_listings.extend(valid_listings)

                logger.info(f"   ðŸ“Š {len(valid_listings)} annonces")

            except Exception as e:
                logger.error(f"   âŒ {scraper_name}: {str(e)[:50]}")
                continue

        # Traitement des nouvelles annonces
        new_count = 0

        for listing in all_listings:
            if self._matches_criteria(listing):
                if not db.listing_exists(listing['listing_id']):
                    if db.add_listing(listing):
                        logger.info(f"ðŸŽ‰ NOUVELLE ANNONCE")
                        logger.info(f"   ðŸ“ {listing['title'][:50]}...")
                        logger.info(f"   ðŸ’° {listing['price']}â‚¬ | ðŸ›ï¸ {listing['rooms']} | ðŸ“ {listing['city']}")

                        # Envoyer notification
                        if notifier.send_listing(listing):
                            db.mark_as_notified(listing['listing_id'])
                            new_count += 1
                            logger.info(f"   ðŸ“¤ Notification envoyÃ©e")
                        else:
                            logger.warning(f"   âš ï¸ Ã‰chec notification")

        # RÃ©sumÃ©
        stats = db.get_stats()

        logger.info(f"\nðŸ“Š RÃ‰SUMÃ‰ CYCLE #{self.cycle_count}")
        logger.info(f"{'-'*40}")
        logger.info(f"ðŸ“ˆ Annonces trouvÃ©es: {len(all_listings)}")
        logger.info(f"ðŸ†• Nouvelles annonces: {new_count}")
        logger.info(f"ðŸ—„ï¸  Base de donnÃ©es: {stats.get('total', 0)} annonces")
        logger.info(f"âœ… NotifiÃ©es: {stats.get('notified', 0)}")

        return new_count

    def _matches_criteria(self, listing):
        """CritÃ¨res de base"""
        try:
            price = listing.get('price', 0)
            if price > MAX_PRICE or price <= 0:
                return False

            rooms = listing.get('rooms', 0)
            if rooms < MIN_ROOMS:
                return False

            return True
        except:
            return False

    def run_once(self):
        """Test unique"""
        print(f"\n{'='*60}")
        print("ðŸ§ª TEST UNIQUE - PHASE 2")
        print(f"{'='*60}")

        new_count = self.check_new_listings()

        print(f"\n{'='*60}")
        print(f"âœ… TERMINÃ‰: {new_count} nouvelle(s) annonce(s)")
        print(f"{'='*60}")

        return new_count

    def run_continuous(self):
        """Mode production"""
        logger.info("ðŸš€ DÃ‰MARRAGE EN CONTINU...")

        # Message de dÃ©marrage
        message = f"""
ðŸ¤– *BOT IMMOBILIER DÃ‰MARRÃ‰ - PHASE 2*

ðŸ“Š *Sites actifs:* {len(self.scrapers)}
ðŸ’° *Prix max:* {MAX_PRICE}â‚¬
ðŸ›ï¸ *PiÃ¨ces min:* {MIN_ROOMS}
â° *Cycle:* {CHECK_INTERVAL//60} minutes

âœ… *Sites:*
â€¢ Athome.lu
â€¢ Immotop.lu
â€¢ Luxhome.lu
â€¢ VIVI.lu
â€¢ Newimmo.lu
â€¢ Unicorn.lu
        """
        notifier.send_message(message)

        try:
            while True:
                self.check_new_listings()

                wait_min = CHECK_INTERVAL // 60
                logger.info(f"\nâ³ Prochain cycle dans {wait_min} minutes...")
                time.sleep(CHECK_INTERVAL)

        except KeyboardInterrupt:
            logger.info("\nâ¹ï¸ ArrÃªt manuel")
            notifier.send_message("â¹ï¸ *Bot arrÃªtÃ©*")
            db.close()
        except Exception as e:
            logger.error(f"âŒ Erreur: {e}")
            notifier.send_message(f"ðŸš¨ *Erreur:* {str(e)[:50]}")
            db.close()

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--once', action='store_true', help='Test unique')

    args = parser.parse_args()

    bot = ImmoBot()

    if args.once:
        bot.run_once()
    else:
        bot.run_continuous()

if __name__ == "__main__":
    main()
