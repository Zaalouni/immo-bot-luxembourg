
# scrapers/athome_scraper.py - VERSION CORRIGÃ‰E
import requests
import logging
import re
import time
import random
from bs4 import BeautifulSoup
from config import USER_AGENT, MAX_PRICE, MIN_ROOMS, CITIES

logger = logging.getLogger(__name__)

class AthomeScraper:
    """Scraper pour athome.lu avec contournement anti-bot"""

    def __init__(self):
        self.base_url = 'https://www.athome.lu'
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
        }

        # Sessions pour garder les cookies
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def get_with_retry(self, url, params=None, max_retries=3):
        """GET avec retry et dÃ©lais alÃ©atoires"""
        for attempt in range(max_retries):
            try:
                # DÃ©lai alÃ©atoire entre les requÃªtes
                delay = random.uniform(2, 5)
                time.sleep(delay)

                # Ajouter headers dynamiques
                dynamic_headers = {
                    'Referer': self.base_url,
                    'X-Requested-With': 'XMLHttpRequest' if attempt > 0 else None,
                }

                headers = {**self.headers, **dynamic_headers}

                response = self.session.get(
                    url,
                    params=params,
                    headers=headers,
                    timeout=30,
                    allow_redirects=True
                )

                # VÃ©rifier si bloquÃ©
                if response.status_code == 403 or "access denied" in response.text.lower():
                    logger.warning(f"   âš ï¸  AccÃ¨s bloquÃ© (403), tentative {attempt+1}/{max_retries}")
                    time.sleep(random.uniform(5, 10))
                    continue

                response.raise_for_status()
                return response

            except requests.exceptions.HTTPError as e:
                if response.status_code == 500:
                    logger.warning(f"   âš ï¸  Erreur 500, tentative {attempt+1}/{max_retries}")
                    time.sleep(random.uniform(5, 10))
                    continue
                elif response.status_code == 429:  # Too Many Requests
                    wait_time = random.uniform(30, 60)
                    logger.warning(f"   âš ï¸  Rate limit (429), attente {wait_time:.1f}s")
                    time.sleep(wait_time)
                    continue
                else:
                    raise e
            except Exception as e:
                logger.error(f"   âŒ Erreur requÃªte: {e}")
                if attempt == max_retries - 1:
                    raise

        return None

    def scrape(self):
        """Scraper les annonces Athome.lu - version robuste"""
        listings = []

        try:
            logger.info("ðŸ” Scraping Athome.lu (version robuste)...")

            # URL plus simple et gÃ©nÃ©rale
            search_url = f"{self.base_url}/louer/appartement"

            # ParamÃ¨tres simples
            params = {
                'size': '100',
                'page': '1',
                'sort': 'date_desc'
            }

            # Essayer une requÃªte gÃ©nÃ©rale d'abord
            try:
                response = self.get_with_retry(search_url, params)
                if not response:
                    logger.error("   âŒ Ã‰chec aprÃ¨s plusieurs tentatives")
                    return []

                logger.info(f"   âœ… Page chargÃ©e ({len(response.content)} bytes)")

                # Analyser le contenu
                soup = BeautifulSoup(response.content, 'html.parser')

                # MÃ©thode 1: Chercher par structure commune
                listings_data = self._extract_from_page(soup)

                if listings_data:
                    listings.extend(listings_data)
                    logger.info(f"   ðŸ“Š {len(listings_data)} annonces extraites")

                # MÃ©thode 2: Chercher donnÃ©es JSON dans la page
                json_listings = self._extract_json_data(response.text)
                if json_listings:
                    listings.extend(json_listings)
                    logger.info(f"   ðŸ“Š +{len(json_listings)} annonces depuis JSON")

                # Filtrer doublons
                unique_ids = set()
                unique_listings = []
                for listing in listings:
                    if listing['listing_id'] not in unique_ids:
                        unique_ids.add(listing['listing_id'])
                        unique_listings.append(listing)

                listings = unique_listings

                # Filtrer par villes si spÃ©cifiÃ©es
                if CITIES:
                    filtered_listings = []
                    cities_lower = [c.lower() for c in CITIES]
                    for listing in listings:
                        city = listing.get('city', '').lower()
                        if any(target_city in city for target_city in cities_lower):
                            filtered_listings.append(listing)

                    listings = filtered_listings

                logger.info(f"âœ… Athome.lu: {len(listings)} annonces valides aprÃ¨s filtrage")
                return listings

            except Exception as e:
                logger.error(f"   âŒ Erreur scraping gÃ©nÃ©ral: {e}")
                return []

        except Exception as e:
            logger.error(f"âŒ Erreur scraping Athome.lu: {e}")
            return []

    def _extract_from_page(self, soup):
        """Extraire annonces depuis le HTML"""
        listings = []

        # DiffÃ©rents sÃ©lecteurs possibles (athome change souvent)
        selectors = [
            'article[class*="property"]',
            'div[class*="listing"]',
            'div[data-testid*="property"]',
            'a[href*="/annonce/"]',
            '.property-item',
            '.srp-list-item',
            '[data-id]'
        ]

        for selector in selectors:
            try:
                elements = soup.select(selector)
                if elements and len(elements) > 2:  # Au moins quelques Ã©lÃ©ments
                    logger.info(f"   ðŸ”Ž SÃ©lecteur '{selector}': {len(elements)} Ã©lÃ©ments")

                    for elem in elements[:20]:  # Limiter pour test
                        listing = self._parse_element(elem)
                        if listing:
                            listings.append(listing)

                    if listings:
                        break  # SÃ©lecteur valide trouvÃ©
            except Exception as e:
                continue

        return listings

    def _extract_json_data(self, html):
        """Extraire donnÃ©es depuis scripts JSON"""
        import json
        listings = []

        try:
            # Chercher des patterns JSON dans la page
            patterns = [
                r'window\.__INITIAL_STATE__\s*=\s*({.*?});',
                r'"properties"\s*:\s*(\[.*?\])',
                r'"items"\s*:\s*(\[.*?\])',
                r'"listings"\s*:\s*(\[.*?\])',
            ]

            for pattern in patterns:
                matches = re.findall(pattern, html, re.DOTALL)
                for match in matches:
                    try:
                        data = json.loads(match)
                        listings_from_json = self._parse_json_data(data)
                        if listings_from_json:
                            listings.extend(listings_from_json)
                    except:
                        continue

        except Exception as e:
            logger.debug(f"   âš ï¸  Erreur extraction JSON: {e}")

        return listings

    def _parse_json_data(self, data):
        """Parser donnÃ©es JSON"""
        listings = []

        def extract_from_dict(obj, path=""):
            results = []

            if isinstance(obj, dict):
                # VÃ©rifier si cet objet ressemble Ã  une annonce
                if any(key in obj for key in ['id', 'price', 'title', 'city']):
                    listing = self._parse_json_listing(obj)
                    if listing:
                        results.append(listing)

                # Explorer rÃ©cursivement
                for key, value in obj.items():
                    results.extend(extract_from_dict(value, f"{path}.{key}"))

            elif isinstance(obj, list):
                for item in obj:
                    results.extend(extract_from_dict(item, path))

            return results

        return extract_from_dict(data)

    def _parse_json_listing(self, data):
        """Parser une annonce depuis JSON"""
        try:
            listing_id = str(data.get('id') or data.get('propertyId') or '')
            if not listing_id:
                return None

            title = data.get('title') or data.get('name') or 'Sans titre'
            price = data.get('price') or data.get('priceAmount') or 0
            if isinstance(price, dict):
                price = price.get('amount', 0)

            city = data.get('city') or data.get('location', {}).get('city', '')
            rooms = data.get('bedrooms') or data.get('roomCount') or 0
            surface = data.get('surface') or data.get('livingArea') or 0

            # URL
            url_slug = data.get('slug') or data.get('url') or f'/annonce/{listing_id}'
            url = f"{self.base_url}{url_slug}" if url_slug.startswith('/') else url_slug

            return {
                'listing_id': f'athome_{listing_id}',
                'site': 'Athome.lu',
                'title': str(title)[:100],
                'city': str(city),
                'price': int(price) if price else 0,
                'rooms': int(rooms) if rooms else 0,
                'surface': int(surface) if surface else 0,
                'url': url,
                'time_ago': 'rÃ©cent'
            }

        except Exception as e:
            logger.debug(f"   âš ï¸  Erreur parsing JSON listing: {e}")
            return None

    def _parse_element(self, elem):
        """Parser un Ã©lÃ©ment HTML"""
        try:
            # Extraire ID
            elem_id = elem.get('id') or elem.get('data-id') or ''
            if not elem_id:
                # Essayer de trouver un ID dans les enfants
                id_elem = elem.find(attrs={'data-id': True})
                if id_elem:
                    elem_id = id_elem.get('data-id', '')

            if not elem_id:
                return None

            # Titre
            title_elem = elem.find(['h2', 'h3', 'h4']) or elem.find(attrs={'class': re.compile(r'title|name')})
            title = title_elem.text.strip() if title_elem else 'Sans titre'

            # Prix
            price_elem = elem.find(attrs={'class': re.compile(r'price|amount|â‚¬')})
            price = 0
            if price_elem:
                price_text = price_elem.text.strip()
                price_match = re.search(r'(\d[\d\s]*)', price_text.replace('.', '').replace(',', ''))
                if price_match:
                    price = int(price_match.group(1).replace(' ', ''))

            # Ville
            city_elem = elem.find(attrs={'class': re.compile(r'city|location|place')})
            city = city_elem.text.strip() if city_elem else ''

            # Chambres
            rooms_elem = elem.find(attrs={'class': re.compile(r'bedroom|room|chambre')})
            rooms = 0
            if rooms_elem:
                rooms_text = rooms_elem.text.strip()
                rooms_match = re.search(r'(\d+)', rooms_text)
                if rooms_match:
                    rooms = int(rooms_match.group(1))

            # Surface
            surface_elem = elem.find(attrs={'class': re.compile(r'surface|area|mÂ²')})
            surface = 0
            if surface_elem:
                surface_text = surface_elem.text.strip()
                surface_match = re.search(r'(\d+)', surface_text)
                if surface_match:
                    surface = int(surface_match.group(1))

            # URL
            link_elem = elem.find('a', href=True)
            url = '#'
            if link_elem:
                href = link_elem['href']
                if href and not href.startswith('javascript'):
                    url = href if href.startswith('http') else f"{self.base_url}{href}"

            return {
                'listing_id': f'athome_{elem_id}',
                'site': 'Athome.lu',
                'title': title[:100],
                'city': city,
                'price': price,
                'rooms': rooms,
                'surface': surface,
                'url': url,
                'time_ago': 'rÃ©cent'
            }

        except Exception as e:
            logger.debug(f"   âš ï¸  Erreur parsing Ã©lÃ©ment: {e}")
            return None

    def _matches_criteria(self, listing):
        """VÃ©rifier si annonce correspond aux critÃ¨res"""
        try:
            # Prix
            price = listing.get('price', 0)
            if price > MAX_PRICE or price <= 0:
                return False

            # Chambres
            rooms = listing.get('rooms', 0)
            if rooms < MIN_ROOMS:
                return False

            return True

        except Exception as e:
            logger.error(f"   âŒ Erreur vÃ©rification critÃ¨res: {e}")
            return False


# Instance globale
athome_scraper = AthomeScraper()
