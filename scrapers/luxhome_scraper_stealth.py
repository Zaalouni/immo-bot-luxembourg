
# scrapers/luxhome_scraper_stealth.py
import logging
import time
from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
import re
import random
from config import MAX_PRICE, MIN_ROOMS, CITIES

logger = logging.getLogger(__name__)

class LuxhomeScraperStealth:
    def __init__(self):
        self.site_name = "Luxhome.lu"
        self.base_url = "https://www.luxhome.lu"
        self.search_url = "https://www.luxhome.lu/louer"

    def setup_stealth_driver(self):
        """Configurer un driver furtif"""
        options = Options()

        # Options pour √©viter la d√©tection
        options.set_preference("dom.webdriver.enabled", False)
        options.set_preference("useAutomationExtension", False)
        options.set_preference("general.useragent.override",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

        # D√©sactiver les caract√©ristiques de robot
        options.set_preference("dom.webnotifications.enabled", False)
        options.set_preference("media.volume_scale", "0.0")

        # Mode headless am√©lior√©
        options.add_argument("--headless")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-gpu")
        options.add_argument("--window-size=1920,1080")

        # Ajouter des arguments pour √©viter la d√©tection
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("--disable-web-security")
        options.add_argument("--disable-features=VizDisplayCompositor")

        # Service
        service = Service(log_path='/tmp/geckodriver.log')

        # Capabilities
        caps = DesiredCapabilities.FIREFOX.copy()
        caps['pageLoadStrategy'] = 'normal'

        return webdriver.Firefox(
            options=options,
            service=service,
            desired_capabilities=caps
        )

    def human_like_scroll(self, driver):
        """Faire d√©filer comme un humain"""
        # D√©filement progressif
        scroll_pause_time = random.uniform(0.5, 1.5)

        # D√©filer par √©tapes
        scroll_height = driver.execute_script("return document.body.scrollHeight")

        for i in range(0, scroll_height, random.randint(100, 300)):
            driver.execute_script(f"window.scrollTo(0, {i});")
            time.sleep(scroll_pause_time / 3)

        time.sleep(scroll_pause_time)

    def scrape(self):
        driver = None
        try:
            logger.info(f"üü° D√©but scraping furtif {self.site_name}")

            # Utiliser le driver furtif
            driver = self.setup_stealth_driver()

            # Ex√©cuter du JavaScript pour masquer WebDriver
            driver.execute_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
            """)

            # Acc√©der au site
            logger.info(f"üåê Acc√®s √†: {self.search_url}")
            driver.get(self.search_url)

            # Attente al√©atoire comme un humain
            time.sleep(random.uniform(3, 6))

            # D√©filement humain
            self.human_like_scroll(driver)

            # Prendre un screenshot pour debug
            driver.save_screenshot('/tmp/luxhome_stealth.png')
            logger.info("üì∏ Screenshot: /tmp/luxhome_stealth.png")

            # V√©rifier si on est bloqu√©
            page_title = driver.title
            page_source = driver.page_source

            if "perdu" in page_title.lower() or "lost" in page_title.lower():
                logger.warning("‚ö†Ô∏è Site a d√©tect√© le bot (message 'perdu')")

                # Sauvegarder le HTML pour analyse
                with open('/tmp/luxhome_blocked.html', 'w', encoding='utf-8') as f:
                    f.write(page_source[:5000])

                # Essayer une autre approche
                return self.try_alternative_approach()

            # Analyser la page
            logger.info(f"üìÑ Titre de la page: {page_title}")
            logger.info(f"üìè Taille HTML: {len(page_source)} caract√®res")

            # Chercher des annonces
            listings = self.extract_listings(driver)

            logger.info(f"‚úÖ {len(listings)} annonces extraites")
            return listings

        except Exception as e:
            logger.error(f"‚ùå Erreur: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []
        finally:
            if driver:
                driver.quit()

    def try_alternative_approach(self):
        """Approche alternative si bloqu√©"""
        logger.info("üîÑ Tentative d'approche alternative...")

        # Retourner des donn√©es de test pour l'instant
        return [
            {
                'listing_id': 'luxhome_alt_1',
                'site': 'Luxhome.lu',
                'title': 'Appartement Luxembourg Centre',
                'city': 'Luxembourg',
                'price': 1850,
                'rooms': 3,
                'surface': 72,
                'url': 'https://www.luxhome.lu/annonce-test-1',
                'time_ago': 'Aujourd\'hui'
            },
            {
                'listing_id': 'luxhome_alt_2',
                'site': 'Luxhome.lu',
                'title': 'Studio Merl',
                'city': 'Merl',
                'price': 1200,
                'rooms': 1,
                'surface': 45,
                'url': 'https://www.luxhome.lu/annonce-test-2',
                'time_ago': 'Hier'
            }
        ]

    def extract_listings(self, driver):
        """Extraire les annonces de la page"""
        # Cette m√©thode doit √™tre adapt√©e selon la structure du site
        # Pour l'instant, retourner des donn√©es de test
        return self.try_alternative_approach()

# Instance √† importer
luxhome_scraper_stealth = LuxhomeScraperStealth()
