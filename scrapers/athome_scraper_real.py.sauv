import requests
import re
import logging
from config import MAX_PRICE, MIN_ROOMS

logger = logging.getLogger(__name__)

class AthomeScraperReal:
    def __init__(self):
        self.base_url = "https://www.athome.lu"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def scrape(self):
        try:
            url = f"{self.base_url}/location"
            response = requests.get(url, headers=self.headers, timeout=15)
            
            if response.status_code != 200:
                logger.error(f"HTTP {response.status_code}")
                return []
            
            html = response.text
            
            # Extraire blocs annonces (id + prix + permalink)
            pattern = r'"id":(\d{7,}).*?"price":(\d+).*?"permalink":\{[^}]*"fr":"([^"]+)"'
            matches = re.findall(pattern, html, re.DOTALL)
            
            logger.info(f"Annonces trouvées: {len(matches)}")
            
            listings = []
            for id_val, price, url_path in matches[:10]:
                # Décoder unicode
                url_clean = url_path.replace('\\u002F', '/')
                
                # Filtrer prix
                price_int = int(price)
                if price_int > MAX_PRICE or price_int <= 0:
                    continue
                
                listing = {
                    'listing_id': f'athome_{id_val}',
                    'site': 'Athome.lu',
                    'title': f'Annonce Athome {id_val}',
                    'city': self._extract_city(url_clean),
                    'price': price_int,
                    'rooms': 2,  # Par défaut
                    'surface': 65,
                    'url': f"{self.base_url}{url_clean}",
                    'time_ago': "Récemment"
                }
                listings.append(listing)
            
            logger.info(f"✅ {len(listings)} annonces après filtrage")
            return listings
            
        except Exception as e:
            logger.error(f"❌ Scraping: {e}")
            return []
    
    def _extract_city(self, url_path):
        parts = url_path.split('/')
        if len(parts) >= 4:
            city = parts[3].replace('-', ' ').title()
            return city
        return 'Luxembourg'

athome_scraper_real = AthomeScraperReal()
