# üéØ PLAN D'ACTIONS COMPLET ‚Äî Immo-Bot Luxembourg

> **Fichier central de planification**
> Quoi faire, en d√©tail, avec le besoin et les files concern√©s.

---

## üé® Vue globale du projet

### ‚ùì Vos besoins

1. **Donn√©es correctes** : Prix, chambres, surface, URL, photos ‚úÖ
2. **Dates de publication** : Savoir si annonce est nouvelle (< 1h), hier, ou ancienne
3. **Dashboard** : Voir annonces tri√©es par DATE, avec filtres (24h, 48h, 7j)
4. **Bot 2x/jour** : Lancer minimum 2 fois par jour, capturer les annonces fra√Æches

### ‚úÖ Ce qu'on a cr√©√©

| Fichier cr√©√© | R√¥le | Pages |
|--------------|------|-------|
| **TEST_AND_FIX_GUIDE.md** | Corriger bugs prix/chambres | 5 |
| **PUBLICATION_DATE_GUIDE.md** | Ajouter dates + fallback | 10 |
| **test_scrapers_quality.py** | Tests qualit√© donn√©es | 400 lignes |
| **test_price_parsing.py** | Tests parsing prix | 400 lignes |
| **scrapers_analysis.md** | Analyse 7 scrapers | 15 pages |
| **SCRAPERS_BUGS_REPORT.md** | Rapport bugs d√©taill√© | 4 pages |
| **PLAN_ACTIONS_COMPLET.md** | **CE FICHIER** | Roadmap d√©taill√©e |

---

## üìä DASHBOARD ACTUEL ‚Äî √âtat et corrections

### ‚ö†Ô∏è Probl√®me : 3 scripts redondants

Il existe **3 dashboards** qui ne sont pas int√©gr√©s au bot principal :

| Script | Type | √âtat | Probl√®me |
|--------|------|------|---------|
| **dashboard_generator.py** | PWA standalone | ‚úÖ Meilleur | HTML inline 450L, lancement manuel |
| **dashboard.py** | Console + HTML | ‚ö†Ô∏è Legacy | Redondant, HTML vieux |
| **web_dashboard.py** | Flask server | ‚ùå Non utilis√© | Flask absent, jamais appel√© |

**Solution** : Utiliser **dashboard_generator.py** + l'am√©liorer avec **published_at**

### üìã Am√©liorations dashboard (Phase 3)

**√Ä ajouter √† dashboard_generator.py** :

```python
# 1. Modifier requ√™te SQL pour inclure published_at
SELECT listing_id, site, title, city, price, rooms, surface,
       url, latitude, longitude, distance_km, created_at, published_at
FROM listings
ORDER BY published_at DESC  # ‚Üê Tri par date publication

# 2. Ajouter published_at + time_ago aux exports JSON/JS
"published_at": "2026-02-26T09:45:00",
"time_ago": "5 min",

# 3. Modifier template HTML: ajouter colonne "Publi√©"
<th>üìÖ Publi√©</th>
<td>${formatTime(l.time_ago)}</td>

# 4. Trier tableau par d√©faut par published_at DESC
listings.sort((a,b) => new Date(b.published_at) - new Date(a.published_at))
```

**√Ä ajouter √† main.py** :

```python
# Import
from dashboard_generator import generate_dashboard

# Apr√®s scraping:
def check_new_listings(self):
    # ... scraping code ...

    # √Ä la fin: rafra√Æchir dashboard automatiquement
    logger.info("Rafra√Æchissement du dashboard...")
    generate_dashboard()  # ‚Üê Appel√© automatiquement 2x/jour
```

**R√©sultat** :
- ‚úÖ Dashboard rafra√Æchi automatiquement (2x/jour via main.py)
- ‚úÖ Annonces tri√©es par published_at (nouvelles en haut)
- ‚úÖ Colonne "Publi√© il y a X min/h/j"
- ‚úÖ Filtres et carte toujours disponibles

---

## üìã Roadmap : 3 √©tapes principales

```
√âTAPE 1: Corriger bugs prix/chambres (PRIORIT√â HAUTE) ‚Äî 20 min
‚î£‚îÅ 2 bugs confirm√©s : VIVI + Immotop
‚î£‚îÅ Fichier: TEST_AND_FIX_GUIDE.md
‚îó‚îÅ R√©sultat: Donn√©es correctes

√âTAPE 2: Ajouter dates publication (PRIORIT√â MOYENNE) ‚Äî 10h
‚î£‚îÅ published_at pour 7 scrapers + BD + fallback
‚î£‚îÅ Fichier: PUBLICATION_DATE_GUIDE.md
‚îó‚îÅ R√©sultat: Chaque annonce a published_at

√âTAPE 3: Int√©grer dashboard (PRIORIT√â HAUTE) ‚Äî 2h
‚î£‚îÅ Am√©liorer dashboard_generator.py
‚î£‚îÅ Appel automatique depuis main.py
‚îó‚îÅ R√©sultat: Dashboard rafra√Æchi 2x/jour, tri√© par date
```

**TOTAL: 12.5 heures (~1.5 jours)**

---

## üöÄ √âTAPE 1 : Corriger bugs prix/chambres

### ‚ùì Pourquoi c'est critique

- Vous avez des **fausses annonces** avec prix/chambres incorrects
- 2 bugs confirm√©s par tests : VIVI (loyer vs charges), Immotop (‚Ç¨ non nettoy√©)
- 4 bugs th√©oriques identifi√©s : Luxhome, Newimmo, Unicorn, Athome
- **Cons√©quence** : Annonces filtr√©es ou accept√©es avec mauvaises donn√©es

### üìÅ Fichiers concern√©s

```
scrapers/vivi_scraper_selenium.py      ‚Üê Bug #1 (VIVI): loyer vs charges
scrapers/immotop_scraper_real.py       ‚Üê Bug #2 (Immotop): ‚Ç¨ non nettoy√©
scrapers/luxhome_scraper.py            ‚Üê Bug #3 (th√©orique): format mixte
scrapers/newimmo_scraper_real.py       ‚Üê Bug #4: d√©cimal mixte
scrapers/unicorn_scraper_real.py       ‚Üê Bug #4: d√©cimal mixte
scrapers/athome_scraper_json.py        ‚Üê Bug #1: prix dict imbriqu√©
utils.py                               ‚Üê √Ä ajouter: parse_price_robust()

Tests:
test_price_parsing.py                  ‚Üê Valider apr√®s corrections
test_scrapers_quality.py               ‚Üê Valider avec donn√©es r√©elles
```

### üìã √Ä faire : Plan d√©taill√© (Phase 1-3 de TEST_AND_FIX_GUIDE.md)

#### **Phase 1.1 : Corriger VIVI (Bug #1) ‚Äî 5 min**

**Le bug** :
```
Texte: "Charges: 150‚Ç¨\nLoyer: 1250‚Ç¨"
R√©sultat: Capture 150‚Ç¨ au lieu de 1250‚Ç¨ ‚ùå
```

**Fichier** : `scrapers/vivi_scraper_selenium.py` ligne 123-133

**Action** :
```bash
# 1. Ouvrir fichier
nano scrapers/vivi_scraper_selenium.py

# 2. Trouver section _extract_listing() ligne 123
#    Chercher boucle: for line in text.split('\n'):

# 3. Remplacer par (chercher "loyer" sp√©cifiquement):
price = 0

# √âtape 1: Chercher ligne avec "loyer"
for line in text.split('\n'):
    if '‚Ç¨' in line and 'loyer' in line.lower():
        price_digits = re.sub(r'[^\d]', '', line)
        if price_digits:
            try:
                price = int(price_digits)
                break
            except ValueError:
                continue

# √âtape 2: Fallback si pas trouv√©
if price == 0:
    for line in text.split('\n'):
        if '‚Ç¨' in line and not any(kw in line.lower() for kw in ['charge', 'd√©p√¥t', 'frais']):
            price_digits = re.sub(r'[^\d]', '', line)
            if price_digits:
                try:
                    price = int(price_digits)
                    if price > 100:
                        break
                except ValueError:
                    continue

# 4. Valider
python test_price_parsing.py TestPriceParsing.test_vivi_charges_before_loyer
# ‚úÖ Attendre: PASSED

# 5. Committer
git add scrapers/vivi_scraper_selenium.py
git commit -m "Fix VIVI bug #1: chercher 'loyer' sp√©cifiquement"
```

---

#### **Phase 1.2 : Corriger Immotop (Bug #2) ‚Äî 5 min**

**Le bug** :
```
Input: "1 250‚Ç¨"
Traitement: int("1250‚Ç¨") ‚Üí ValueError ‚ùå
R√©sultat: Prix = 0, annonce rejet√©e
```

**Fichier** : `scrapers/immotop_scraper_real.py` ligne 85

**Action** :
```bash
# 1. Ouvrir fichier
nano scrapers/immotop_scraper_real.py

# 2. Trouver ligne 85 dans _scrape()
#    Chercher: price_clean = price_text.replace(' ', '')

# 3. Remplacer:
# Avant:
price_clean = price_text.replace(' ', '').replace('\u202f', '').replace(',', '')

# Apr√®s (ajouter .replace('‚Ç¨', '')):
price_clean = price_text.replace(' ', '').replace('\u202f', '').replace(',', '').replace('‚Ç¨', '')

# 4. Valider
python test_price_parsing.py TestPriceParsing.test_immotop_euro_symbol
# ‚úÖ Attendre: PASSED

# 5. Committer
git add scrapers/immotop_scraper_real.py
git commit -m "Fix Immotop bug #2: ajouter .replace('‚Ç¨', '')"
```

---

#### **Phase 1.3 : Validation compl√®te ‚Äî 10 min**

**Action** :
```bash
# 1. Lancer tous les tests parsing
python test_price_parsing.py
# ‚úÖ Attendre: 23/23 PASSED (ou au minimum 21/23)

# 2. Optionnel: Tester sur donn√©es r√©elles
timeout 600 python test_scrapers_quality.py --all
# ‚úÖ Attendre: Moins d'erreurs prix

# 3. V√©rifier logs
python3 << 'EOF'
from scrapers.vivi_scraper_selenium import vivi_scraper_selenium
listings = vivi_scraper_selenium.scrape()
if listings:
    print(f"‚úÖ VIVI: {len(listings)} annonces")
    print(f"Prix range: {min(l['price'] for l in listings)} - {max(l['price'] for l in listings)}")
    print(f"Prix = 0: {len([l for l in listings if l['price'] == 0])}")
EOF

# 4. Committer validation
git add test_price_parsing.py
git commit -m "Phase 1 validated: 2 critical bugs fixed, 23/23 tests passing"

# 5. OPTIONNEL Phase 2 (robustesse): Cr√©er parse_price_robust()
# ‚Üí Voir TEST_AND_FIX_GUIDE.md Phase 2
```

---

### ‚úÖ **R√©sultat Phase 1**

- ‚úÖ Bug #1 (VIVI) corrig√©
- ‚úÖ Bug #2 (Immotop) corrig√©
- ‚úÖ Tests parsing 23/23 passants
- ‚úÖ Annonces VIVI + Immotop : donn√©es correctes

### ‚è±Ô∏è Temps total Phase 1 : **20 min**

---

## üóìÔ∏è √âTAPE 2 : Ajouter dates de publication

### ‚ùì Pourquoi c'est important

- Vous lancez bot **2x/jour minimum**
- Besoin de savoir : **"C'est une annonce nouvelle ?"**
- Sans date : impossible de filtrer les annonces fra√Æches
- Avec date : dashboard peut trier par date, notifier si < 1h, etc.

### üìÅ Fichiers concern√©s

```
utils.py                              ‚Üê √Ä ajouter: ensure_published_at()
database.py                           ‚Üê √Ä modifier: add column published_at
scrapers/athome_scraper_json.py       ‚Üê √Ä modifier: ajouter published_at
scrapers/immotop_scraper_real.py      ‚Üê √Ä modifier: ajouter published_at
scrapers/luxhome_scraper.py           ‚Üê √Ä modifier: ajouter published_at
scrapers/vivi_scraper_selenium.py     ‚Üê √Ä modifier: ajouter published_at
scrapers/nextimmo_scraper.py          ‚Üê √Ä modifier: ajouter published_at
scrapers/newimmo_scraper_real.py      ‚Üê √Ä modifier: ajouter published_at
scrapers/unicorn_scraper_real.py      ‚Üê √Ä modifier: ajouter published_at

Tests:
test_date_parsing.py                  ‚Üê √Ä cr√©er (valider parsing date)
test_date_quality.py                  ‚Üê √Ä cr√©er (valider qualit√© date)
```

### üìã √Ä faire : Plan d√©taill√© (Phase 1-7 de PUBLICATION_DATE_GUIDE.md)

#### **Phase 2.1 : Analyser sources date ‚Äî 2h**

**Objectif** : Savoir o√π est la date dans chaque scraper

**Action** :
```bash
# Pour chaque scraper, tester manuellement

# 1. ATHOME
python3 << 'EOF'
from scrapers.athome_scraper_json import athome_scraper_json
import json
listings = athome_scraper_json.scrape()
if listings:
    print("=== ATHOME ===")
    item = listings[0]
    print(f"Cl√©s disponibles: {list(item.keys())}")
    # Chercher cl√© date: publishDate, createdAt, timestamp, time_ago
    for key in ['publishDate', 'createdAt', 'timestamp', 'time_ago', 'date', 'created']:
        if key in item:
            print(f"‚úÖ TROUV√â: {key} = {item[key]}")
EOF

# 2. NEXTIMMO
python3 << 'EOF'
from scrapers.nextimmo_scraper import nextimmo_scraper
import json
listings = nextimmo_scraper.scrape()
if listings:
    print("=== NEXTIMMO ===")
    item = listings[0]
    print(f"Cl√©s disponibles: {list(item.keys())}")
    for key in ['createdAt', 'publishedAt', 'timestamp', 'date', 'published', 'created']:
        if key in item:
            print(f"‚úÖ TROUV√â: {key} = {item[key]}")
EOF

# 3-7. M√™me chose pour IMMOTOP, LUXHOME, VIVI, NEWIMMO, UNICORN

# R√©sultat attendu: Document "DATE_SOURCES.txt" listant cl√© + format
# Ex:
# Athome: publishDate (ISO 8601)
# Nextimmo: createdAt (ISO 8601)
# Immotop: time_ago (texte "il y a 2h")
# VIVI: datetime attribute dans HTML
# etc.
```

**R√©sultat** : Cr√©er `DATE_SOURCES.txt` avec tableau r√©capitulatif

---

#### **Phase 2.2 : Cr√©er fonctions utilitaires ‚Äî 1h**

**Fichier** : `utils.py`

**Action** :
```bash
# 1. Ajouter √† la fin de utils.py :

cat >> utils.py << 'EOF'

# ===== DATE PARSING =====
from datetime import datetime, timedelta

def ensure_published_at(published_at=None):
    """
    ‚≠ê FONCTION CRITIQUE
    Garantir que published_at JAMAIS None.
    Fallback central pour tous les scrapers.
    """
    if published_at is None:
        return datetime.now()
    if not isinstance(published_at, datetime):
        return datetime.now()
    now = datetime.now()
    if published_at > now:
        return now  # Correction futur
    return published_at

def parse_relative_date(text):
    """Parser "il y a X jours" ‚Üí datetime"""
    if not text:
        return None
    text_lower = text.lower()

    if 'r√©cemment' in text_lower or 'aujourd' in text_lower:
        return datetime.now()

    match = re.search(r'(\d+)\s*(?:heure|jour|semaine|mois)s?', text_lower)
    if match:
        number = int(match.group(1))
        if 'heure' in match.group(0):
            return datetime.now() - timedelta(hours=number)
        elif 'jour' in match.group(0):
            return datetime.now() - timedelta(days=number)
        elif 'semaine' in match.group(0):
            return datetime.now() - timedelta(weeks=number)
        elif 'mois' in match.group(0):
            return datetime.now() - timedelta(days=number*30)
    return None

def parse_iso_date(date_str):
    """Parser ISO 8601 (ex: "2026-02-26T09:45:00Z")"""
    try:
        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
    except:
        return None

def parse_absolute_date(date_str, format_str="%d/%m/%Y"):
    """Parser date absolue (ex: "26/02/2026")"""
    try:
        return datetime.strptime(date_str, format_str)
    except:
        return None
EOF

# 2. Tester
python3 << 'EOF'
from utils import ensure_published_at, parse_relative_date, parse_iso_date
from datetime import datetime

tests = [
    (ensure_published_at(None), datetime.now(), "Fallback None"),
    (parse_relative_date("il y a 2 heures"), lambda: datetime.now() - timedelta(hours=2), "Relative 2h"),
    (parse_iso_date("2026-02-26T09:45:00Z"), datetime(2026, 2, 26, 9, 45), "ISO date"),
]

print("‚úÖ Fonctions date cr√©√©es dans utils.py")
EOF

# 3. Committer
git add utils.py
git commit -m "Phase 2.2: Add date parsing functions (ensure_published_at, parse_*)"
```

---

#### **Phase 2.3 : Modifier base de donn√©es ‚Äî 30 min**

**Fichier** : `database.py`

**Action** :
```bash
# 1. Ajouter colonne published_at dans init_db()
nano database.py

# Chercher: CREATE TABLE IF NOT EXISTS listings (
# Ajouter apr√®s notified:
    published_at TIMESTAMP,  # ‚Üê NOUVELLE

# Ajouter index:
self.cursor.execute('''
    CREATE INDEX IF NOT EXISTS idx_published_at
    ON listings(published_at)
''')

# 2. Modifier add_listing() pour accepter published_at
# Chercher: def add_listing(self, listing_data):
# Modifier INSERT pour inclure published_at

# Avant:
INSERT INTO listings
(listing_id, site, title, city, price, rooms, surface, url, latitude, longitude, distance_km)
VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)

# Apr√®s:
INSERT INTO listings
(listing_id, site, title, city, price, rooms, surface, url, latitude, longitude, distance_km, published_at)
VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)

# Ajouter dans VALUES tuple:
listing_data.get('published_at')  # ‚Üê √Ä la fin

# 3. Ajouter m√©thode get_listings_by_date()
# Voir code dans PUBLICATION_DATE_GUIDE.md section "Nouvelle m√©thode pour query par date"

# 4. Tester
python3 << 'EOF'
from database import Database
db = Database()
print("‚úÖ Database sch√©ma mis √† jour")
EOF

# 5. Migrate DB existante (optionnel)
sqlite3 listings.db << 'EOF'
ALTER TABLE listings ADD COLUMN published_at TIMESTAMP;
CREATE INDEX idx_published_at ON listings(published_at);
EOF

# 6. Committer
git add database.py
git commit -m "Phase 2.3: Add published_at column + index to DB"
```

---

#### **Phase 2.4 : Modifier 7 scrapers ‚Äî 3h (30 min chacun)**

**Pattern pour tous les scrapers** :

```python
# Au d√©but du fichier scraper, ajouter import:
from utils import ensure_published_at, parse_relative_date, parse_iso_date
from datetime import datetime

# Dans _extract_listing() ou √©quivalent, √Ä LA FIN avant return:

# √âtape 1: Chercher date de publication
published_at = None
# [Code sp√©cifique au scraper pour extraire date]

# √âtape 2: FALLBACK GARANTI
published_at = ensure_published_at(published_at)

# Dans return dict:
'published_at': published_at  # ‚Üê JAMAIS None
```

**Action par scraper** :

```bash
# 1. NEXTIMMO (API propre, easy) ‚Äî 20 min
nano scrapers/nextimmo_scraper.py
# Dans _extract_from_json(), avant return:
# Chercher: createdAt ou publishedAt dans item
# Code exemple:
date_str = item.get('createdAt') or item.get('publishedAt')
published_at = None
if date_str:
    published_at = parse_iso_date(date_str)
published_at = ensure_published_at(published_at)
# Ajouter au return: 'published_at': published_at

# 2. IMMOTOP (texte visible) ‚Äî 20 min
nano scrapers/immotop_scraper_real.py
# Dans scrape(), chercher texte "il y a X jours"
# Utiliser parse_relative_date()

# 3. VIVI (Selenium + HTML) ‚Äî 20 min
nano scrapers/vivi_scraper_selenium.py
# Dans _extract_listing(), chercher <time> ou attribut data-date

# 4. ATHOME (JSON) ‚Äî 20 min
nano scrapers/athome_scraper_json.py
# Chercher publishDate, createdAt, timestamp dans item

# 5. LUXHOME (JSON/regex) ‚Äî 15 min
nano scrapers/luxhome_scraper.py
# Chercher cl√© date dans regex match

# 6. NEWIMMO (regex HTML) ‚Äî 15 min
nano scrapers/newimmo_scraper_real.py
# Chercher "Publi√© le DD/MM/YYYY" dans contexte

# 7. UNICORN (regex HTML) ‚Äî 15 min
nano scrapers/unicorn_scraper_real.py
# Chercher <time> ou "depuis X jours"

# Apr√®s chaque modification:
python test_price_parsing.py  # V√©rifier parsing OK
python3 << 'EOF'
from scrapers.xxx_scraper import xxx_scraper
listings = xxx_scraper.scrape()
if listings:
    print(f"‚úÖ {len(listings)} listings avec published_at")
    print(f"Sample: {listings[0].get('published_at')}")
EOF

# Committer par scraper
git add scrapers/xxx_scraper.py
git commit -m "Phase 2.4: Add published_at extraction to xxx_scraper"
```

---

#### **Phase 2.5-2.7 : Tests + Validation**

```bash
# Phase 2.5: Tests qualit√© (1h)
python test_date_quality.py  # √Ä cr√©er
# V√©rifier: tous les listings ont published_at
# V√©rifier: published_at <= now()
# V√©rifier: pas de dates trop anciennes (> 30j)

# Phase 2.6: Dashboard (2h)
# Cr√©er dashboard.py avec:
# - get_listings_by_date(hours=24)
# - HTML table tri√©e par date d√©croissante
# - Filter buttons: 24h, 48h, 7j

# Phase 2.7: Validation finale (30 min)
git status
sqlite3 listings.db "SELECT COUNT(*), COUNT(published_at) FROM listings;"
# ‚úÖ Tous les listings ont published_at
```

---

### ‚úÖ **R√©sultat Phase 2**

- ‚úÖ Fonction `ensure_published_at()` en utils.py
- ‚úÖ BD + colonne `published_at` + index
- ‚úÖ 7 scrapers modi avec extraction date + fallback
- ‚úÖ Dashboard affichant annonces tri√©es par date
- ‚úÖ Filtres : 24h, 48h, 7 jours

### ‚è±Ô∏è Temps total Phase 2 : **10 heures**

---

## üìä √âTAPE 3 : Int√©grer et am√©liorer dashboard

### ‚ùì Pourquoi c'est utile

- Dashboard **existant** (dashboard_generator.py) ‚Äî le meilleur des 3
- Am√©liorer avec **published_at** (dates de publication)
- Appel **automatique** depuis main.py (2x/jour)
- Vue centralis√©e, tri√©e par date, avec filtres

### üìÅ Fichiers concern√©s

```
dashboard_generator.py     ‚Üê √Ä modifier (ajouter published_at)
main.py                    ‚Üê √Ä modifier (appel automatique)
dashboards/index.html      ‚Üê Sera r√©g√©n√©r√©
dashboards/data/
‚îú‚îÄ‚îÄ listings.js            ‚Üê Inclura published_at
‚îî‚îÄ‚îÄ listings.json
```

### üìã √Ä faire : Plan d√©taill√©

#### **Phase 3.1 : Am√©liorer dashboard_generator.py**

```python
# Cr√©er fichier: dashboard.py

from database import db
from datetime import datetime, timedelta

def get_dashboard_listings(filter_hours=24):
    """R√©cup√©rer annonces pour dashboard"""
    listings = db.get_listings_by_date(hours=filter_hours, limit=100)

    result = []
    for listing in listings:
        listing_id, site, title, city, price, published_at = listing

        # Calculer temps √©coul√©
        if published_at:
            delta = datetime.now() - published_at
            if delta.total_seconds() < 60:
                time_str = "√Ä l'instant"
            elif delta.total_seconds() < 3600:
                time_str = f"{int(delta.total_seconds() / 60)} min"
            elif delta.total_seconds() < 86400:
                time_str = f"{int(delta.total_seconds() / 3600)}h"
            else:
                time_str = f"{int(delta.total_seconds() / 86400)} j"
        else:
            time_str = "N/A"

        result.append({
            'listing_id': listing_id,
            'site': site,
            'title': title,
            'city': city,
            'price': price,
            'published_at': published_at,
            'time_str': time_str,
        })

    return result

def render_html(filter_hours=24):
    """G√©n√©rer HTML du dashboard"""
    listings = get_dashboard_listings(filter_hours)

    html = f"""
    <html>
    <head>
        <title>Immo-Bot Dashboard</title>
        <link rel="stylesheet" href="dashboard_style.css">
    </head>
    <body>
        <h1>üìä Dashboard Annonces Immobili√®res</h1>

        <div class="filters">
            <a href="?filter=24" class="{'active' if filter_hours == 24 else ''}">24h</a>
            <a href="?filter=48" class="{'active' if filter_hours == 48 else ''}">48h</a>
            <a href="?filter=168" class="{'active' if filter_hours == 168 else ''}">7 jours</a>
        </div>

        <table class="listings">
            <thead>
                <tr>
                    <th>üìÖ Publi√©</th>
                    <th>Site</th>
                    <th>Titre</th>
                    <th>Ville</th>
                    <th>üí∞ Prix</th>
                </tr>
            </thead>
            <tbody>
    """

    for listing in listings:
        html += f"""
                <tr class="site-{listing['site'].lower().replace('.', '')}">
                    <td><span title="{listing['published_at']}">{listing['time_str']}</span></td>
                    <td>{listing['site']}</td>
                    <td>{listing['title']}</td>
                    <td>{listing['city']}</td>
                    <td>{listing['price']}‚Ç¨</td>
                </tr>
        """

    html += """
            </tbody>
        </table>
    </body>
    </html>
    """
    return html
```

---

#### **Phase 3.2 : Cr√©er dashboard_style.css**

```css
body {
    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
    max-width: 1400px;
    margin: 0 auto;
    padding: 20px;
    background-color: #f5f5f5;
}

h1 {
    color: #333;
    text-align: center;
    border-bottom: 3px solid #007bff;
    padding-bottom: 10px;
}

.filters {
    margin: 20px 0;
    text-align: center;
}

.filters a {
    padding: 10px 20px;
    margin: 0 5px;
    background: #f0f0f0;
    text-decoration: none;
    border-radius: 4px;
    color: #333;
    transition: all 0.3s;
}

.filters a.active {
    background: #007bff;
    color: white;
    font-weight: bold;
}

table.listings {
    width: 100%;
    border-collapse: collapse;
    background: white;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    border-radius: 4px;
    overflow: hidden;
}

table.listings th {
    background: #007bff;
    color: white;
    padding: 15px;
    text-align: left;
    font-weight: bold;
}

table.listings td {
    padding: 12px 15px;
    border-bottom: 1px solid #eee;
}

table.listings tr:hover {
    background: #f9f9f9;
}

/* Highlight nouvelles annonces (< 1h) */
table.listings tr:has(td:first-child span[title*="202"]) {
    background: #fff3cd;
    border-left: 4px solid #ffc107;
}

/* Colorer par site */
tr.site-athomelulocalStorage { border-left: 4px solid #004E89; }
tr.site-immotoplulocalStorage { border-left: 4px solid #FF6B35; }
tr.site-luxhomelulocalStorage { border-left: 4px solid #118AB2; }
tr.site-vivilulocalStorage { border-left: 4px solid #A23B72; }
tr.site-nextimmolulocalStorage { border-left: 4px solid #073B4C; }
tr.site-newimlolulocalStorage { border-left: 4px solid #006E90; }
tr.site-unicornlulocalStorage { border-left: 4px solid #F77F00; }
```

---

#### **Phase 3.3 : Int√©grer dans main.py**

```bash
# Modifier main.py pour ajouter route dashboard

# √Ä ajouter:
from dashboard import render_html
from flask import Flask, request

app = Flask(__name__)

@app.route('/dashboard')
def dashboard():
    filter_hours = int(request.args.get('filter', 24))
    return render_html(filter_hours)

# Optionnel: afficher aussi dans logs
def check_new_listings():
    listings = dashbaord.get_dashboard_listings(hours=1)  # < 1h
    logger.info(f"üÜï {len(listings)} annonces dans derni√®re heure")
```

---

### ‚úÖ **R√©sultat Phase 3**

- ‚úÖ Dashboard HTML affichant annonces tri√©es par date
- ‚úÖ Filtres : 24h, 48h, 7 jours
- ‚úÖ Highlighting annonces r√©centes (< 1h)
- ‚úÖ Styling professionnel

### ‚è±Ô∏è Temps total Phase 3 : **3 heures**

---

## üìÖ Timeline compl√®te

```
LUNDI:
  ‚îú‚îÄ Matin (1h) : Corriger bugs VIVI + Immotop
  ‚îú‚îÄ Apr√®s-midi (1.5h) : Lancer tests qualit√©
  ‚îî‚îÄ Total : 2.5h ‚Üí Phase 1 COMPL√àTE ‚úÖ

MARDI + MERCREDI:
  ‚îú‚îÄ 2h : Analyser sources date (Phase 2.1)
  ‚îú‚îÄ 1h : Cr√©er fonctions date (Phase 2.2)
  ‚îú‚îÄ 30min : Modifier BD (Phase 2.3)
  ‚îú‚îÄ 3h : Modifier 7 scrapers (Phase 2.4)
  ‚îú‚îÄ 2.5h : Tests + Validation (2.5-2.7)
  ‚îî‚îÄ Total : 10h ‚Üí Phase 2 COMPL√àTE ‚úÖ

JEUDI:
  ‚îú‚îÄ 1h : Cr√©er dashboard.py
  ‚îú‚îÄ 30min : CSS styling
  ‚îú‚îÄ 30min : Int√©grer main.py
  ‚îú‚îÄ 1h : Tests finaux
  ‚îî‚îÄ Total : 3h ‚Üí Phase 3 COMPL√àTE ‚úÖ

TOTAL: 15.5 heures (~2 jours complets)
```

---

## üéÅ R√©sultat final

### Dashboard avec annonces tri√©es par date

```
üìä Immo-Bot Dashboard

[Filtres: 24h | 48h | 7j]

| üìÖ Publi√© | Site     | Titre                      | Ville       | Prix  |
|-----------|----------|--------------------------|-------------|-------|
| 5 min    | Athome   | 2ch, 75m¬≤, lumineux       | Luxembourg  | 1250‚Ç¨ |
| 45 min   | Nextimmo | 3ch, 90m¬≤, proche train   | Esch        | 1800‚Ç¨ |
| 2h       | VIVI     | 2ch, 80m¬≤, balcon         | Differdange | 1500‚Ç¨ |
| 8h       | Luxhome  | 1ch, 45m¬≤, calme          | Dudelange   | 900‚Ç¨  |
| 1j       | Immotop  | 2ch, 65m¬≤, parking incl   | Luxembourg  | 1400‚Ç¨ |
```

### Notification Telegram enrichie

```
üè† Nouvelle annonce!
   Publi√©e il y a 5 minutes ‚è∞

üìå 2 chambres, 1250‚Ç¨/mois
   75 m¬≤, Luxembourg

üîó Athome.lu
```

---

## üíæ Commandes r√©capitulatif

```bash
# Phase 1: Bugs prix (20 min)
nano scrapers/vivi_scraper_selenium.py      # Fix loyer vs charges
nano scrapers/immotop_scraper_real.py       # Fix ‚Ç¨ symbol
python test_price_parsing.py                # Valider
git commit -m "Phase 1: Fix price bugs"

# Phase 2: Dates (10h)
nano utils.py                               # Add date functions
nano database.py                            # Add published_at column
# For each of 7 scrapers:
nano scrapers/xxx_scraper.py                # Add published_at
python test_date_quality.py                 # Valider
git commit -m "Phase 2: Add publication dates"

# Phase 3: Dashboard (3h)
cat > dashboard.py << 'EOF'...             # Create dashboard
cat > dashboard_style.css << 'EOF'...      # Create CSS
nano main.py                               # Add /dashboard route
git commit -m "Phase 3: Create dashboard"

# Final
git log --oneline -10                      # V√©rifier commits
git push origin claude/dashboard-analysis-docs-FLiRU
```

---

## üöÄ Commencer maintenant

### Option A : Faire tout (15.5h)
```
Jour 1: Phase 1 (bugs prix) + Phase 2 (dates)
Jour 2: Phase 3 (dashboard)
R√©sultat: Syst√®me complet
```

### Option B : Faire progressivement
```
Semaine 1: Phase 1 (corriger bugs)
Semaine 2: Phase 2 (ajouter dates)
Semaine 3: Phase 3 (cr√©er dashboard)
```

### Option C : Prioriser urgent
```
Urgent: Phase 1 (20 min) ‚Äî Corriger fausses donn√©es
Ensuite: Phase 2 (10h) ‚Äî Ajouter dates
Optionnel: Phase 3 (3h) ‚Äî Dashboard (peut rester simple)
```

---

## üìû Questions fr√©quentes

**Q: Par o√π commencer ?**
A: Phase 1 (20 min) pour corriger bugs, puis Phase 2 (10h) pour dates.

**Q: Et si je n'ai pas 15h ?**
A: Phase 1 seul (20 min) donne d√©j√† des donn√©es correctes.

**Q: Comment valider chaque phase ?**
A: Tests fournis : test_price_parsing.py, test_date_quality.py, etc.

**Q: Puis-je faire phases 1+2 en parall√®le ?**
A: Oui, mais Phase 1 d'abord (plus rapide et urgent).

---

**Cr√©√©** : 2026-02-26
**Derni√®re mise √† jour** : 2026-02-26
**Statut** : Pr√™t pour ex√©cution

